{"cells":[{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from efficientnet_pytorch import EfficientNet\n","from torchvision.transforms import RandomErasing"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on CPU\n"]}],"source":["# Check if CUDA is available\n","if torch.cuda.is_available():\n","    # Set the default tensor type to CUDA tensors\n","    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","\n","    # Define the device as the first visible cuda device if available\n","    device = torch.device(\"cuda:0\")\n","    print(f\"Running on {torch.cuda.get_device_name(device)}\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on CPU\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Data augmentation for the training set\n","train_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n","])\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["train_dataset_path = \"Dataset/train\"\n","val_dataset_path = \"Dataset/validation\"\n","test_dataset_path = \"Dataset/test\" \n","\n","# Data loaders\n","train_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","# Load datasets\n","train_dataset = datasets.ImageFolder(train_dataset_path, transform=train_transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8, generator=torch.Generator(device))\n","\n","val_dataset = datasets.ImageFolder(val_dataset_path, transform=train_transform)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=8)\n","\n","test_dataset = datasets.ImageFolder(test_dataset_path, transform=train_transform)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to /Users/maryam/.cache/torch/hub/checkpoints/efficientnet-b3-5fb5a3c3.pth\n","100%|██████████| 47.1M/47.1M [00:01<00:00, 40.4MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded pretrained weights for efficientnet-b3\n"]}],"source":["# Model\n","model_name = 'efficientnet-b3'\n","model = EfficientNet.from_pretrained(model_name)\n","\n","# Adding Dropout layer\n","dropout_rate = 0.5\n","model._dropout = nn.Dropout(p=dropout_rate)\n","\n","# Modify the Final Layer for binary classification\n","num_classes = 2\n","in_features = model._fc.in_features\n","model._fc = nn.Linear(in_features, num_classes)\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Loss and Optimizer with L2 Regularization (Weight Decay)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/5], Step [100/3469], Loss: 0.0799\n","Epoch [1/5], Step [200/3469], Loss: 0.0060\n","Epoch [1/5], Step [300/3469], Loss: 0.1495\n","Epoch [1/5], Step [400/3469], Loss: 0.0156\n","Epoch [1/5], Step [500/3469], Loss: 0.2717\n","Epoch [1/5], Step [600/3469], Loss: 0.0462\n","Epoch [1/5], Step [700/3469], Loss: 0.1839\n","Epoch [1/5], Step [800/3469], Loss: 0.1122\n","Epoch [1/5], Step [900/3469], Loss: 0.0513\n","Epoch [1/5], Step [1000/3469], Loss: 0.0150\n","Epoch [1/5], Step [1100/3469], Loss: 0.0185\n","Epoch [1/5], Step [1200/3469], Loss: 0.0966\n","Epoch [1/5], Step [1300/3469], Loss: 0.0800\n","Epoch [1/5], Step [1400/3469], Loss: 0.1016\n","Epoch [1/5], Step [1500/3469], Loss: 0.0374\n","Epoch [1/5], Step [1600/3469], Loss: 0.0568\n","Epoch [1/5], Step [1700/3469], Loss: 0.0077\n","Epoch [1/5], Step [1800/3469], Loss: 0.0233\n","Epoch [1/5], Step [1900/3469], Loss: 0.1228\n","Epoch [1/5], Step [2000/3469], Loss: 0.0115\n","Epoch [1/5], Step [2100/3469], Loss: 0.0101\n","Epoch [1/5], Step [2200/3469], Loss: 0.0480\n","Epoch [1/5], Step [2300/3469], Loss: 0.0273\n","Epoch [1/5], Step [2400/3469], Loss: 0.1277\n","Epoch [1/5], Step [2500/3469], Loss: 0.0374\n"]},{"ename":"FileNotFoundError","evalue":"Caught FileNotFoundError in DataLoader worker process 6.\nOriginal Traceback (most recent call last):\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torchvision/datasets/folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n             ^^^^^^^^^^^^^^^^^\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torchvision/datasets/folder.py\", line 268, in default_loader\n    return pil_loader(path)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torchvision/datasets/folder.py\", line 246, in pil_loader\n    with open(path, \"rb\") as f:\n         ^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'Dataset/train/Real/real_12393.jpg'\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):  \n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/starter_updated/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/starter_updated/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/starter_updated/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m~/starter_updated/.venv/lib/python3.11/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 6.\nOriginal Traceback (most recent call last):\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torchvision/datasets/folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n             ^^^^^^^^^^^^^^^^^\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torchvision/datasets/folder.py\", line 268, in default_loader\n    return pil_loader(path)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/maryam/starter_updated/.venv/lib/python3.11/site-packages/torchvision/datasets/folder.py\", line 246, in pil_loader\n    with open(path, \"rb\") as f:\n         ^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'Dataset/train/Real/real_12393.jpg'\n"]}],"source":["# Training Loop\n","num_epochs = 5\n","for epoch in range(num_epochs):  \n","    model.train()\n","    for i, (images, labels) in enumerate(train_loader):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n","\n","    # Save the model at the end of each epoch\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss.item(),\n","    }, f\"model_epoch_{epoch+1}.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_val_accuracy = 0\n","\n","# Validation Loop\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in val_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    current_val_accuracy = 100 * correct / total\n","    print(f\"Validation Accuracy: {100 * correct / total}%\")\n","\n","    # Save the model if it has the best validation accuracy so far\n","    if current_val_accuracy > best_val_accuracy:\n","        best_val_accuracy = current_val_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Test Loop with Confusion Matrix\n","model.eval()\n","all_labels = []\n","all_preds = []\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        all_labels.extend(labels.cpu().numpy())\n","        all_preds.extend(predicted.cpu().numpy())\n","\n","conf_mat = confusion_matrix(all_labels, all_preds)\n","sns.heatmap(conf_mat, annot=True, fmt=\"d\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","plt.show()\n","\n","# Calculate and print test accuracy\n","correct = sum([1 for i, j in zip(all_labels, all_preds) if i == j])\n","total = len(all_labels)\n","print(f\"Test Accuracy: {100 * correct / total}%\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1909705,"sourceId":3134515,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
